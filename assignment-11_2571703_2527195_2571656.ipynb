{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11: Sequence Learning: Recurrent and Recursive Neural Networks (deadline: 27 Jan, 23:59)\n",
    "\n",
    "For theoretical tasks you are encouraged to write in $\\\\LaTeX$. Jupyter notebooks support them by default. For reference, please have a look at the examples in this short excellent guide: [Typesetting Equations](http://nbviewer.jupyter.org/github/ipython/ipython/blob/3.x/examples/Notebook/Typesetting%20Equations.ipynb)\n",
    "\n",
    "Alternatively, you can upload the solutions in the written form as images and paste them inside the cells. But if you do this, **make sure** that the images are of high quality, so that we can read them without any problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Comparing Vanilla RNN and LSTM for different Sequence Lengths (7 points)\n",
    "\n",
    "**Goal**: To study the variation of training performance for different sequence lengths in Vanilla RNN and Long-Short Term Memory (LSTM) Neural Networks, on a word prediction task.\n",
    "\n",
    "For this exercise, you will need to familiarize yourself with LSTMs. A good tutorial on LSTMs is presented at [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "The following LSTM tensorflow code is derived from an example [here](https://github.com/roatienza/Deep-Learning-Experiments/blob/master/Experiments/Tensorflow/RNN/rnn_words.py). This code allows you to run an LSTM-based Neural Network on a word prediction task. The learning is set up to predict the next word given the previous `n_input` words.\n",
    "\n",
    "You will be using this code, the file `train.txt` from NNIA's resources page on Piazza and answering the following questions to complete this exercise. \n",
    "\n",
    "Note: You will need tensorflow installed to your IPython Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n",
      "n_input is: 3\n",
      "Iter= 1000, Average Loss= 5.923856, Average Accuracy= 4.50%\n",
      "['which', 'to', 'live.'] - [but] vs [the]\n",
      "Iter= 2000, Average Loss= 4.120572, Average Accuracy= 10.60%\n",
      "['fled', 'to', 'the'] - [forest.] vs [forest.]\n",
      "Iter= 3000, Average Loss= 3.139697, Average Accuracy= 15.20%\n",
      "['after', 'the', 'latter'] - [had] vs [and]\n",
      "Iter= 4000, Average Loss= 3.019088, Average Accuracy= 21.90%\n",
      "['lion', 'lying', 'down'] - [moaning] vs [the]\n",
      "Iter= 5000, Average Loss= 2.638940, Average Accuracy= 31.50%\n",
      "['the', 'emperor', 'and'] - [all] vs [all]\n",
      "Training Finished!\n",
      "Elapsed time:  55.163323163986206 sec\n",
      "n_input is: 7\n",
      "Iter= 1000, Average Loss= 6.637741, Average Accuracy= 7.60%\n",
      "['and', 'the', 'lion', 'were', 'captured,', 'and', 'the'] - [slave] vs [causing]\n",
      "Iter= 2000, Average Loss= 3.578212, Average Accuracy= 24.80%\n",
      "['near,', 'the', 'lion', 'put', 'out', 'his', 'paw,'] - [which] vs [which]\n",
      "Iter= 3000, Average Loss= 2.158090, Average Accuracy= 53.00%\n",
      "['lion', 'was', 'let', 'loose', 'from', 'his', 'den,'] - [and] vs [to]\n",
      "Iter= 4000, Average Loss= 1.068563, Average Accuracy= 76.00%\n",
      "['causing', 'all', 'the', 'pain.', 'he', 'pulled', 'out'] - [the] vs [the]\n",
      "Iter= 5000, Average Loss= 0.865951, Average Accuracy= 80.50%\n",
      "['to', 'androcles', 'he', 'recognised', 'his', 'friend,', 'and'] - [fawned] vs [the]\n",
      "Training Finished!\n",
      "Elapsed time:  1.7365672906239829 min\n",
      "n_input is: 10\n",
      "Iter= 1000, Average Loss= 6.621048, Average Accuracy= 5.30%\n",
      "['surprised', 'at', 'this,', 'summoned', 'androcles', 'to', 'him,', 'who', 'told', 'him'] - [the] vs [the]\n",
      "Iter= 2000, Average Loss= 3.917344, Average Accuracy= 16.70%\n",
      "['the', 'emperor,', 'surprised', 'at', 'this,', 'summoned', 'androcles', 'to', 'him,', 'who'] - [told] vs [loose]\n",
      "Iter= 3000, Average Loss= 2.266285, Average Accuracy= 48.20%\n",
      "['him', 'the', 'whole', 'story.', 'whereupon', 'the', 'slave', 'was', 'pardoned', 'and'] - [freed,] vs [freed,]\n",
      "Iter= 4000, Average Loss= 0.844851, Average Accuracy= 79.00%\n",
      "['was', 'pardoned', 'and', 'freed,', 'and', 'the', 'lion', 'let', 'loose', 'to'] - [his] vs [his]\n",
      "Iter= 5000, Average Loss= 0.575878, Average Accuracy= 87.10%\n",
      "['into', 'it,', 'and', 'was', 'causing', 'all', 'the', 'pain.', 'he', 'pulled'] - [out] vs [out]\n",
      "Training Finished!\n",
      "Elapsed time:  2.3417255679766336 min\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Recurrent Neural Network (LSTM) implementation example using TensorFlow..\n",
    "Next word prediction after n_input words learned from text file.\n",
    "A story is automatically generated if the predicted word is fed back as input.\n",
    "\n",
    "Source Author: Rowel Atienza\n",
    "Project: https://github.com/roatienza/Deep-Learning-Experiments\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\"\n",
    "\n",
    "# Target log path\n",
    "logs_path = './rnn_words/lstm'\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "# Text file containing words for training\n",
    "training_file = 'train.txt'\n",
    "\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "dictionary, reverse_dictionary = build_dataset(training_data)\n",
    "vocab_size = len(dictionary)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 5000\n",
    "display_step = 1000\n",
    "#n_input = 3\n",
    "n_inputs = [3,7,10]\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "for n_input in n_inputs:\n",
    "    print(\"n_input is:\", n_input)\n",
    "    with tf.Graph().as_default():\n",
    "        # tf Graph input\n",
    "        x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "        y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "        # RNN output node weights and biases\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "        }\n",
    "\n",
    "        def RNN(x, weights, biases):\n",
    "\n",
    "            # reshape to [1, n_input]\n",
    "            x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "            # Generate a n_input-element sequence of inputs\n",
    "            # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "            x = tf.split(x,n_input,1)\n",
    "\n",
    "            # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "            # TODO replace the following layer with a Vanilla RNN tf.contrib.rnn call\n",
    "            rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "            # generate prediction\n",
    "            outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "            # there are n_input outputs but\n",
    "            # we only want the last output\n",
    "            return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "        pred = RNN(x, weights, biases)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the Session\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            step = 0\n",
    "            offset = random.randint(0,n_input+1)\n",
    "            end_offset = n_input + 1\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "\n",
    "            writer.add_graph(session.graph)\n",
    "\n",
    "            while step < training_iters:\n",
    "                # Generate a minibatch. Add some randomness on selection process.\n",
    "                if offset > (len(training_data)-end_offset):\n",
    "                    offset = random.randint(0, n_input+1)\n",
    "\n",
    "                symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "                symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "                symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "                symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "                symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "                _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "                loss_total += loss\n",
    "                acc_total += acc\n",
    "                if (step+1) % display_step == 0:\n",
    "                    print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                          \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                    acc_total = 0\n",
    "                    loss_total = 0\n",
    "                    symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "                    symbols_out = training_data[offset + n_input]\n",
    "                    symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                    print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "                step += 1\n",
    "                offset += (n_input+1)\n",
    "            print(\"Training Finished!\")\n",
    "            print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_input is: 3\n",
      "Iter= 1000, Average Loss= 5.960908, Average Accuracy= 4.60%\n",
      "['day', 'used', 'to'] - [bring] vs [victim.]\n",
      "Iter= 2000, Average Loss= 3.910275, Average Accuracy= 14.00%\n",
      "['master', 'and', 'fled'] - [to] vs [the]\n",
      "Iter= 3000, Average Loss= 3.148463, Average Accuracy= 26.60%\n",
      "['the', 'slave', 'was'] - [sentenced] vs [let]\n",
      "Iter= 4000, Average Loss= 3.009698, Average Accuracy= 22.80%\n",
      "['once', 'escaped', 'from'] - [his] vs [his]\n",
      "Iter= 5000, Average Loss= 2.274921, Average Accuracy= 45.50%\n",
      "['from', 'which', 'to'] - [live.] vs [licked]\n",
      "Training Finished!\n",
      "Elapsed time:  1.861034619808197 min\n",
      "n_input is: 7\n",
      "Iter= 1000, Average Loss= 6.181796, Average Accuracy= 7.80%\n",
      "['every', 'day', 'used', 'to', 'bring', 'him', 'meat'] - [from] vs [and]\n",
      "Iter= 2000, Average Loss= 3.249108, Average Accuracy= 30.30%\n",
      "['surprised', 'at', 'this,', 'summoned', 'androcles', 'to', 'him,'] - [who] vs [who]\n",
      "Iter= 3000, Average Loss= 2.169181, Average Accuracy= 53.40%\n",
      "['was', 'sentenced', 'to', 'be', 'thrown', 'to', 'the'] - [lion,] vs [him,]\n",
      "Iter= 4000, Average Loss= 1.307426, Average Accuracy= 74.00%\n",
      "['lion', 'did', 'not', 'pursue', 'him,', 'he', 'turned'] - [back] vs [back]\n",
      "Iter= 5000, Average Loss= 0.668146, Average Accuracy= 84.10%\n",
      "['friend,', 'and', 'fawned', 'upon', 'him,', 'and', 'licked'] - [his] vs [his]\n",
      "Training Finished!\n",
      "Elapsed time:  2.393146594365438 min\n",
      "n_input is: 10\n",
      "Iter= 1000, Average Loss= 6.230342, Average Accuracy= 7.60%\n",
      "['wandering', 'about', 'there', 'he', 'came', 'upon', 'a', 'lion', 'lying', 'down'] - [moaning] vs [was]\n",
      "Iter= 2000, Average Loss= 4.084448, Average Accuracy= 19.70%\n",
      "['forest.', 'as', 'he', 'was', 'wandering', 'about', 'there', 'he', 'came', 'upon'] - [a] vs [back]\n",
      "Iter= 3000, Average Loss= 2.402309, Average Accuracy= 45.80%\n",
      "['like', 'a', 'friendly', 'dog.', 'the', 'emperor,', 'surprised', 'at', 'this,', 'summoned'] - [androcles] vs [androcles]\n",
      "Iter= 4000, Average Loss= 1.454969, Average Accuracy= 70.40%\n",
      "['to', 'androcles', 'he', 'recognised', 'his', 'friend,', 'and', 'fawned', 'upon', 'him,'] - [and] vs [soon]\n",
      "Iter= 5000, Average Loss= 0.419630, Average Accuracy= 90.80%\n",
      "['of', 'the', 'arena.', 'soon', 'the', 'lion', 'was', 'let', 'loose', 'from'] - [his] vs [his]\n",
      "Training Finished!\n",
      "Elapsed time:  3.7064135789871218 min\n"
     ]
    }
   ],
   "source": [
    "# DOING FOR VANILLA RNN CELLS\n",
    "logs_path = './rnn_words/rnn_vanilla'\n",
    "writer = tf.summary.FileWriter(logs_path)\n",
    "\n",
    "for n_input in n_inputs:\n",
    "    print(\"n_input is:\", n_input)\n",
    "    with tf.Graph().as_default():\n",
    "        # tf Graph input\n",
    "        x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "        y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "        # RNN output node weights and biases\n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([vocab_size]))\n",
    "        }\n",
    "\n",
    "        def RNN_Vanilla(x, weights, biases):\n",
    "\n",
    "            # reshape to [1, n_input]\n",
    "            x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "            # Generate a n_input-element sequence of inputs\n",
    "            # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "            x = tf.split(x,n_input,1)\n",
    "\n",
    "            # 1-layer LSTM with n_hidden units but with lower accuracy.\n",
    "            # TODO replace the following layer with a Vanilla RNN tf.contrib.rnn call\n",
    "            #rnn_cell = rnn.BasicLSTMCell(n_hidden)\n",
    "            rnn_cell = rnn.BasicRNNCell(n_hidden)\n",
    "\n",
    "            # generate prediction\n",
    "            outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "            # there are n_input outputs but\n",
    "            # we only want the last output\n",
    "            return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "\n",
    "        pred = RNN(x, weights, biases)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "        # Model evaluation\n",
    "        correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # Launch the Session\n",
    "        with tf.Session() as session:\n",
    "            session.run(init)\n",
    "            step = 0\n",
    "            offset = random.randint(0,n_input+1)\n",
    "            end_offset = n_input + 1\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "\n",
    "            writer.add_graph(session.graph)\n",
    "\n",
    "            while step < training_iters:\n",
    "                # Generate a minibatch. Add some randomness on selection process.\n",
    "                if offset > (len(training_data)-end_offset):\n",
    "                    offset = random.randint(0, n_input+1)\n",
    "\n",
    "                symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "                symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "                symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "                symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "                symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "                _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                        feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "                loss_total += loss\n",
    "                acc_total += acc\n",
    "                if (step+1) % display_step == 0:\n",
    "                    print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                          \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                    acc_total = 0\n",
    "                    loss_total = 0\n",
    "                    symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "                    symbols_out = training_data[offset + n_input]\n",
    "                    symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                    print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "                step += 1\n",
    "                offset += (n_input+1)\n",
    "            print(\"Training Finished!\")\n",
    "            print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) The sequence length used for prediction in the above code is specified by `n_input`. For an LSTM cell, change `n_inputs` to 3, 7, 10 and report training accuracy for each. Note: running this code on a 4GB RAM, Core i3 processor with tensorflow v1.4 takes around four to five minutes. (**1.5 points**)\n",
    "\n",
    "b) In the function `RNN`, replace the LSTM Cell with a Vanilla RNN Cell at `#TODO`. (**1 point**)\n",
    "\n",
    "c) Repeat the experiment in a) for same `n_input` values. (**1.5 points**)\n",
    "\n",
    "d) While comparing Vanilla RNN and LSTM, what trends do you observe with training accuracy when the sequence length is varied? (**1 point**)\n",
    "\n",
    "e) Why do you think one model learns much better than the other?  (**1 point**)\n",
    "\n",
    "f) Do you expect the model with higher training accuracy to generalize well? (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers ###\n",
    "\n",
    "d) For both Vanilla RNN and LSTM, when we increase the sequence length from 3 to 7, the training accuracy improves vastly, and increases from 7 to 10.\n",
    "\n",
    "e) Vanilla RNN shows better training accuracy because it does not have the sigmoid gates that the LSTM cells have. Hence, we do not restrict the flow of information through time steps, so we get better fit on the training data.\n",
    "\n",
    "f) The model with higher training accuracy actually might not generalize well. This is because the LSTM with it's gates that restrict the flow of too much information through time will make the network more generalized to unseen sequences, whereas the vanilla model will pass everything, hence it will tend to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Unfolding Computational Graphs. (6 points)\n",
    "\n",
    "Imagine you build a Vanilla Recurrent Neural Networks with an input layer, a Vanilla RNN based hidden layer and an output layer. \n",
    "\n",
    "a) The backpropagation through time algorithm looks back at a window of 4 previous time steps. Draw this computation as an unfolded graph like Figure 10.3 in the [DL book](http://www.deeplearningbook.org/contents/rnn.html). (**2 points**)\n",
    "\n",
    "b) Which of the weight matrices used in the graph are same? Mark these arrows with the same symbol $W$. (**1 point**)\n",
    "\n",
    "c) This unfolded computational graph for Vanilla RNN can be represented by an equivalent Recursive Neural Network. Draw the architecture for this graph. (**1 point**)\n",
    "\n",
    "d) Can you construct a smaller height Recursive NN than c) with the same coverage of previous time steps? If no, then explain, else if yes, then draw this architecture? (**2 points**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Forget Gate in LSTMs (3 points)\n",
    "\n",
    "LSTMs forget information from its global cell state ($C_t$) that is irrelevant for prediction at the present time step by using the forget gate $f_t$: \n",
    "\n",
    "$C_t = f_t * C_{t-1}+i_t*\\tilde{C_t}$. \n",
    "\n",
    "Refer [Colah's Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) for more information on the notation. \n",
    "\n",
    "As this information ($f_t * C_{t-1}$) is forgotten, it might happen at some time in the future that the prediction process requires this information again, however, as this information is forgotten there is no way to access it again. Suggest a way of saving this information from being forgotten completely? Your solution should work systematically as the LSTM moves over a sequence. \n",
    "\n",
    "Hint: Do you know about Caching Mechanism in physical memories in computers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer ##\n",
    "\n",
    "We may have the network maintain an explicit memory area, where it can store or cache the data and hidden values when the forget cell is forcing the network to \"forget\" that particular data. If the network can learn proper reading and writing to these memory cells, then the network can easily access these memory regions as and when required and use the information. This will allow even longer term dependency, without the training or the network having to face any issues with vanishing gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4. Recurrent and Recursive Neural Networks: Theory (3 points)\n",
    "\n",
    "Following are statements that you should answer with either a True or a False. And, also provide a justification as to why you think so. To answer these questions, you will need to revisit the lecture slides and read the DL book's Chapter on [Sequence Modelling](http://www.deeplearningbook.org/contents/rnn.html).\n",
    "\n",
    "a) A Convolution Neural Network layer forms a shallower way of sharing parameters through time than a Vanilla RNN layer. \\[T/F\\]\n",
    "\n",
    "b) Networks with output recurrence are more powerful than hidden-to-hidden recurrence. \\[T/F\\]\n",
    "\n",
    "c) Removing the Global Cell State from LSTMs will result in a Vanilla RNN. \\[T/F\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers ##\n",
    "\n",
    "a) **TRUE**. A convolution neural network does form a shallower way of sharing parameters through time than a Vanilla RNN layer. In case of a convolution network used for sequential data, like time-delay networks, we apply the same kernel for each time stamp, so at each time stamp, the element depends only on certain of its surrounding elements. The concept of parameter sharing goes only till applying the same kernel to each time stamp. But, for a RNN network, we can share parameters through a very deep computational graph if we consider unfolding of the graphs, and with the fact that the output at a particular time stamp depends on the output of previous time stamps.\n",
    "\n",
    "b) **FALSE**. Networks with output recurrence are less powerful than hidden-to-hidden recurrence. When we have output recurrence, the output will generally be in a much lower dimension than the hidden layer dimension. Hence, it cannot transfer the same amount of rich information from the past through different time-stamps as hidden-hidden recurrence. However, if we have output recurrence, it is usually much easier to train such models.\n",
    "\n",
    "c) **FALSE**. Even if we remove the global cell state, the gates will remain in the LSTM cell, and especially the input gate and the output gates will operate, and that will selectively update the inputs at a time stamp, and pass that to the output but after being moderated by the output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as the solution. The naming should include the assignment number and matriculation IDs of all members in your team in the following format:\n",
    "**assignment-11_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 members in a team). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please submit the solution to your tutor (with **[NNIA][assignment-11]** in email subject):\n",
    "1. Maksym Andriushchenko <s8mmandr@stud.uni-saarland.de>\n",
    "2. Marius Mosbach <s9msmosb@stud.uni-saarland.de>\n",
    "3. Rajarshi Biswas <rbisw17@gmail.com>\n",
    "4. Marimuthu Kalimuthu <s8makali@stud.uni-saarland.de>\n",
    "\n",
    "Note :  **If you are in a team, you should submit only 1 solution to only 1 tutor.** <br>\n",
    "$\\hspace{2em}$ **Submissions violating these rules will not be graded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
